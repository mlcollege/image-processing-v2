{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network on Fashion MNIST\n",
    "---\n",
    "Don't forget to use **https://pytorch.org/docs/stable/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to convolutional filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing_workshop.utils import get_image_from_url\n",
    "from image_processing_workshop.visual import plot_image\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download your favouirite image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://media.wired.com/photos/5bbf72c46278de2d2123485b/master/w_582,c_limit/soyuz-1051882240.jpg'\n",
    "img = get_image_from_url(url, to_grayscale=True)\n",
    "img = img / 255.\n",
    "plot_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore prepared filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_filter = np.array([[-1, -1, 1, 1], \n",
    "                           [-1, -1, 1, 1], \n",
    "                           [-1, -1, 1, 1], \n",
    "                           [-1, -1, 1, 1]])\n",
    "filter_1 = initial_filter\n",
    "filter_2 = -filter_1\n",
    "filter_3 = filter_1.T\n",
    "filter_4 = -filter_3\n",
    "filters = np.array([filter_1, filter_2, filter_3, filter_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(filters[i], cmap='gray')\n",
    "    ax.set_title('Filter %s' % str(i+1))\n",
    "    width, height = filters[i].shape\n",
    "    \n",
    "    # Add -1 1 annotations to image.\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            ax.annotate(str(filters[i][x][y]), xy=(y,x),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center',\n",
    "                        color='white' if filters[i][x][y]<0 else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build small network initialised with those filters\n",
    "In the examples, we will use `torch.nn.conv2d` https://pytorch.org/docs/stable/nn.html#conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PyTorch, we have channels on 1st. So here we have 4 filters, each has 1 channel, all are shape 4x4.\n",
    "filters_torch = torch.from_numpy(filters).unsqueeze(1).type(torch.DoubleTensor)\n",
    "filters_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_torch = torch.from_numpy(img).unsqueeze(0).unsqueeze(1)\n",
    "img_torch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convoluton filters efectively change height and width of input image that\n",
    "\n",
    "$H_{out} = \\lfloor \\frac{H_{in}+2×padding[0]−dilation[0]×(kernel\\_size[0]−1)−1}{stride[0]} +1 \\rfloor$   \n",
    "$W_{out} = \\lfloor \\frac{W_{in}+2×padding[1]−dilation[1]×(kernel\\_size[1]−1)−1}{stride[1]} +1 \\rfloor$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuralNetSimple(nn.Module):    \n",
    "    def __init__(self, filters_torch):\n",
    "        super(ConvNeuralNetSimple, self).__init__()\n",
    "        \n",
    "        height, width = filters_torch.shape[2:]\n",
    "        self.conv_layer = nn.Conv2d(in_channels=1, out_channels=4, \n",
    "                                    kernel_size=(height, width), bias=False)\n",
    "        self.conv_layer.weight.data = filters_torch\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.conv_layer(images)\n",
    "    \n",
    "conv_neural_net_simple = ConvNeuralNetSimple(filters_torch)\n",
    "print(conv_neural_net_simple)\n",
    "print(list(conv_neural_net_simple.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = conv_neural_net_simple(img_torch)\n",
    "feature_maps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of conv layer feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_feature_maps(feature_maps, n_maps= 4):\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    for i in range(n_maps):\n",
    "        ax = fig.add_subplot(1, n_maps, i+1, xticks=[], yticks=[])\n",
    "        # grab layer outputs\n",
    "        ax.imshow(np.squeeze(feature_maps[0,i].data.numpy()), cmap='gray')\n",
    "        ax.set_title('Output %s' % str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source img.\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "# Convolution filters.\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(filters[i], cmap='gray')\n",
    "    ax.set_title('Filter %s' % str(i+1))\n",
    "\n",
    "# Feature maps.    \n",
    "vizualize_feature_maps(feature_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity of image on convolution filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "import ipywidgets as ipw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = feature_maps[0][0].detach().numpy()\n",
    "feature_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(filter_1, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_max = feature_map.max()\n",
    "def plot_sensitivity(tolerance):\n",
    "    feature_map_filtered = (feature_map >= (feature_map_max - tolerance)).astype(int)\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    im = plt.imshow(feature_map_filtered, cmap='gray')\n",
    "    plt.colorbar(im, orientation='horizontal')\n",
    "    plt.gca().axes.set_axis_off()\n",
    "    plt.show()\n",
    "    \n",
    "interactive(plot_sensitivity, tolerance=ipw.FloatSlider(0.5, min=0, max=feature_map_max - 0.1, step=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = Compose([ToTensor()])\n",
    "\n",
    "train_dataset = FashionMNIST('./dataset_fashion_mnist/', download=True, train=True, \n",
    "                             transform=transformations, target_transform=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataset = FashionMNIST('./dataset_fashion_mnist/', download=True, train=False, \n",
    "                             transform=transformations, target_transform=None)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Sequential\n",
    "from torch.nn import ReLU, Tanh, Dropout, Softmax, Linear, Conv2d, MaxPool2d, BatchNorm2d\n",
    "from torch.nn import MSELoss, CrossEntropyLoss, NLLLoss\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn.init import xavier_uniform_, normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        # Variables for logging of layers shapes.\n",
    "        self.use_softmax = False\n",
    "        self.shape_conv1 = None\n",
    "\n",
    "        # 1st segment of conv with batch norm and pooling.\n",
    "        self.conv1 = nn.Sequential(\n",
    "            Conv2d(1, 32, (3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            BatchNorm2d(32),\n",
    "            ReLU(),\n",
    "            MaxPool2d((2, 2), stride=(2, 2)))\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Add another convolution blocks (64 filters) and adjsut Linear part. #\n",
    "        #############################################################################\n",
    "        \n",
    "        # Linear output.\n",
    "        self.linear = Linear(14*14*32, 10)\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.conv1(images)\n",
    "        self.shape_conv1 = x.shape\n",
    "        \n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        if self.use_softmax:\n",
    "            return torch.exp(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "conv_neural_net = ConvNeuralNet()\n",
    "conv_neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = conv_neural_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_neural_net(valid_dataset[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_neural_net.use_softmax = True\n",
    "conv_neural_net(valid_dataset[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_neural_net.use_softmax = False\n",
    "info = conv_neural_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_neural_net.shape_conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and loss function\n",
    "More on loss functions can be found here: https://pytorch.org/docs/stable/nn.html#loss-functions  \n",
    "More on optimizers can be found here: https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fce = NLLLoss()\n",
    "loss_fce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(conv_neural_net.parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_acc_and_loss(model, loss_fce, valid_loader):\n",
    "    accuracy = 0\n",
    "    loss = 0\n",
    "    was_training = model.training\n",
    "    \n",
    "    model.eval()\n",
    "    for images, labels in valid_loader:\n",
    "        predictions = model(images)\n",
    "        accuracy += (predictions.argmax(dim=1) == labels).type(torch.FloatTensor).mean().item() \n",
    "        loss += loss_fce(predictions, labels).item()\n",
    "    model.train(mode=was_training)\n",
    "    return accuracy / len(valid_loader) * 100, loss / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_valid_acc_and_loss(conv_neural_net, loss_fce, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Initial params setup.\n",
    "epochs = 2\n",
    "report_period = 100\n",
    "batch_iteration = 0\n",
    "\n",
    "# Storing of some data.\n",
    "train_leak_loss = deque(maxlen=report_period)\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "valid_acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Setup net to train mode and go through one epoch.\n",
    "    conv_neural_net.train()\n",
    "    for images, labels in train_loader:\n",
    "        batch_iteration += 1\n",
    "        \n",
    "        ##################\n",
    "        # Training Phase #\n",
    "        ##################\n",
    "        optimizer.zero_grad()\n",
    "        predictions = conv_neural_net.forward(images)\n",
    "        loss = loss_fce(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ####################\n",
    "        # Validation Phase #\n",
    "        ####################\n",
    "        train_leak_loss.append(loss.item())\n",
    "        if batch_iteration % report_period == 0:\n",
    "            conv_neural_net.eval()\n",
    "            \n",
    "            # We don't want to collect info for gradients from here.\n",
    "            with torch.no_grad():\n",
    "                valid_accuracy, valid_loss = get_valid_acc_and_loss(conv_neural_net, loss_fce, valid_loader)\n",
    "                \n",
    "            print(f'Epoch: {epoch+1}/{epochs}.. ',\n",
    "                  f\"Train Loss: {round(np.mean(train_leak_loss), 2)}.. \",\n",
    "                  f\"Valid Loss: {round(valid_loss, 2)}.. \",\n",
    "                  f\"Valid Acc: {round(valid_accuracy, 2)}%\")\n",
    "            \n",
    "            train_loss_history.append(np.mean(train_leak_loss))\n",
    "            valid_loss_history.append(valid_loss)\n",
    "            valid_acc_history.append(valid_accuracy)\n",
    "                   \n",
    "            conv_neural_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Cross Entropy')\n",
    "plt.plot(train_loss_history, label='Train loss')\n",
    "plt.plot(valid_loss_history, label='Valid loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.plot(valid_acc_history, label='Valid acc')\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Acc(%)')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_neural_net.eval()\n",
    "conv_neural_net.use_softmax = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View single images and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing_workshop.visual import plot_classify, plot_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classify(valid_dataset[1][0], conv_neural_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load reuslts to pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing_workshop.eval import get_results_df\n",
    "from image_processing_workshop.visual import plot_df_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_results_df(conv_neural_net, valid_loader)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Prediction Score')\n",
    "df[df.label_class_name=='Dress'].label_class_score.hist(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_examples(df.iloc[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing_workshop.eval import get_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_precision(df, 'Dress')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing_workshop.eval import get_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recall(df, 'Dress')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing_workshop.eval import get_rec_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rec_prec(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing_workshop.eval import get_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing_workshop.eval import get_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = get_false_positives(df, label_class_name='Shirt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_examples(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = get_false_positives(df, label_class_name='Shirt', predicted_class_name='Pullover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_examples(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing_workshop.visual import plot_coocurance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coocurance_matrix(df, use_log=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
